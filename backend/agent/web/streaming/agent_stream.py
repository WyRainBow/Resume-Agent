"""Agent stream output handler.

Handles streaming agent execution results to SSE clients.
å½“å‰ä¸»é“¾ä»åŸºäº StreamEvent -> SSE è¾“å‡ºï¼›
CLTP ç›¸å…³èƒ½åŠ›ä½œä¸ºè¿‡æ¸¡å…¼å®¹èµ„äº§é€æ­¥æ”¶æ•›ã€‚
"""

import asyncio
import json
import logging
import re
from dataclasses import dataclass
from typing import Any, AsyncIterator, Callable, Optional, Tuple, List, Set
from datetime import datetime


def parse_thought_response(content: str) -> Tuple[Optional[str], Optional[str]]:
    """
    è§£æ LLM è¾“å‡ºä¸­çš„ Thought å’Œ Response éƒ¨åˆ†

    Deprecated: CLTP å·²æä¾›æ ‡å‡†çš„ think/plain content chunksï¼Œ
    åç»­åœ¨å®Œæˆå‰ç«¯è¿ç§»åç§»é™¤æ­¤å‡½æ•°ä¸ç›¸å…³è°ƒç”¨ã€‚
    TODO(cltp): å‰ç«¯å®Œå…¨è¿ç§»ååˆ é™¤ parse_thought_response

    Returns:
        (thought, response) - å¦‚æœæ²¡æœ‰æ‰¾åˆ°å¯¹åº”éƒ¨åˆ†åˆ™ä¸º None
    """
    # #region debug log (å·²ç¦ç”¨ç¡¬ç¼–ç è·¯å¾„)
    import json
    # ä½¿ç”¨ logger ä»£æ›¿ç¡¬ç¼–ç è·¯å¾„ï¼Œé¿å…åœ¨ä¸åŒç³»ç»Ÿä¸Šå‡ºé”™
    try:
        logger.debug(f"[DEBUG] parse_thought_response called: content_length={len(content) if content else 0}")
    except Exception:
        pass
    # #endregion

    thought = None
    response = None

    if not content or not content.strip():
        """
        # debug log (å·²ç¦ç”¨)
        with open('/Users/wy770/AI/.cursor/debug.log', 'a') as f:
            f.write(json.dumps({
                "sessionId": "debug-session",
                "runId": "run1",
                "hypothesisId": "C",
                "location": "agent_stream.py:parse_thought_response:EMPTY",
                "message": "content is empty or whitespace",
                "data": {"content": content},
                "timestamp": int(__import__('time').time() * 1000)
            }) + '\n')
        """
        return None, None

    # ä½¿ç”¨æ›´ä¸¥è°¨çš„æ­£åˆ™è¡¨è¾¾å¼åŒ¹é… Thought: å’Œ Response:
    # è€ƒè™‘å¯èƒ½å­˜åœ¨çš„æ¢è¡Œå’Œç©ºæ ¼ï¼Œæ”¯æŒå¤šç§æ ¼å¼å˜ä½“
    # åŒ¹é…æ¨¡å¼ï¼š
    # 1. Thought: ... Response: ...
    # 2. **Thought:** ... **Response:** ...
    # 3. Thought: ... (æ²¡æœ‰ Response)
    # 4. æ€è€ƒï¼š... å›å¤ï¼š... (ä¸­æ–‡æ ¼å¼)

    # å°è¯•å¤šç§åŒ¹é…æ¨¡å¼
    patterns = [
        # æ ‡å‡†æ ¼å¼ï¼šThought: ... Response: ...ï¼ˆæ”¯æŒåŒä¸€è¡Œæˆ–æ¢è¡Œï¼‰
        (
            r'(?:^|\n)\s*(?:Thought|æ€è€ƒ)[:ï¼š]\s*(.*?)(?=\s*(?:Response|å›å¤|Answer|Final\s*Answer|æœ€ç»ˆå›å¤)[:ï¼š]|$)',
            r'(?:^|\n|\s)(?:Response|å›å¤|Answer|Final\s*Answer|æœ€ç»ˆå›å¤)[:ï¼š]\s*(.*)',
        ),
        # åŠ ç²—æ ¼å¼ï¼š**Thought:** ... **Response:** ...
        (r'(?:^|\n)\s*\*\*Thought\*\*[:ï¼š]\s*(.*?)(?=\n\s*\*\*Response\*\*[:ï¼š]|$)',
         r'(?:^|\n)\s*\*\*Response\*\*[:ï¼š]\s*(.*)'),
        # 1. Thought: ... 2. Response: ... (å¸¦ç¼–å·)
        (r'(?:^|\n)\s*1\.\s*(?:Thought|æ€è€ƒ)[:ï¼š]\s*(.*?)(?=\n\s*2\.\s*(?:Response|å›å¤)[:ï¼š]|$)',
         r'(?:^|\n)\s*2\.\s*(?:Response|å›å¤)[:ï¼š]\s*(.*)'),
    ]

    for idx, (thought_pattern, response_pattern) in enumerate(patterns):
        thought_match = re.search(thought_pattern, content, re.DOTALL | re.IGNORECASE | re.MULTILINE)
        response_match = re.search(response_pattern, content, re.DOTALL | re.IGNORECASE | re.MULTILINE)

        if thought_match:
            thought = thought_match.group(1).strip()
        if response_match:
            response = response_match.group(1).strip()

        """
        # debug log (å·²ç¦ç”¨)
        if thought_match or response_match:
            with open('/Users/wy770/AI/.cursor/debug.log', 'a') as f:
                f.write(json.dumps({
                    "sessionId": "debug-session",
                    "runId": "run1",
                    "hypothesisId": "B",
                    "location": f"agent_stream.py:parse_thought_response:PATTERN_{idx}",
                    "message": "pattern matched",
                    "data": {
                        "pattern_idx": idx,
                        "thought_matched": thought_match is not None,
                        "response_matched": response_match is not None,
                        "thought_preview": thought[:100] if thought else None,
                        "response_preview": response[:100] if response else None
                    },
                    "timestamp": int(__import__('time').time() * 1000)
                }) + '\n')
        """

        if thought or response:
            break

    # å¯å‘å¼ä¿®å¤ï¼š
    # ä¸€äº›æ¨¡å‹åªè¾“å‡º "Thought: xxx\n<actual answer>"ï¼Œä¸å¸¦ "Response:" æ ‡ç­¾ã€‚
    # è¿™ç§æƒ…å†µä¸‹æŠŠç¬¬ä¸€è¡Œè§†ä½œ thoughtï¼Œå…¶ä½™æ­£æ–‡è§†ä½œ responseï¼Œ
    # é¿å…æœ€ç»ˆ plain å†…å®¹é‡Œæ®‹ç•™ "Thought:" å‰ç¼€ã€‚
    if thought and not response:
        thought_prefix = re.match(
            r"^\s*(?:\*\*)?(?:Thought|æ€è€ƒ)(?:\*\*)?\s*[:ï¼š]\s*",
            content,
            re.IGNORECASE,
        )
        if thought_prefix:
            remaining = content[thought_prefix.end() :].strip()
            # ä¼˜å…ˆæŒ‰ç©ºè¡Œæ‹†åˆ†ï¼Œå¦åˆ™æŒ‰é¦–ä¸ªæ¢è¡Œæ‹†åˆ†
            parts = re.split(r"\n{2,}|\n", remaining, maxsplit=1)
            if len(parts) == 2:
                first_line = parts[0].strip()
                body = parts[1].strip()
                if first_line and body:
                    thought = first_line
                    response = body
            elif remaining:
                # è‡³å°‘ç§»é™¤å‰ç¼€ï¼Œé¿å…åœ¨ plain ä¸­å‡ºç° "Thought:"
                thought = parts[0].strip()
                response = None

    # å¦‚æœæ‰¾åˆ°äº† Thought ä½†æ²¡æ‰¾åˆ° Responseï¼ˆè¿˜åœ¨ç”Ÿæˆä¸­ï¼‰ï¼Œæˆ–è€…æ‰¾åˆ°äº† Response
    if thought or response:
        """
        # debug log (å·²ç¦ç”¨)
        with open('/Users/wy770/AI/.cursor/debug.log', 'a') as f:
            f.write(json.dumps({
                "sessionId": "debug-session",
                "runId": "run1",
                "hypothesisId": "B",
                "location": "agent_stream.py:parse_thought_response:SUCCESS",
                "message": "parse_thought_response success",
                "data": {
                    "thought_found": thought is not None,
                    "response_found": response is not None,
                    "thought_length": len(thought) if thought else 0,
                    "response_length": len(response) if response else 0
                },
                "timestamp": int(__import__('time').time() * 1000)
            }) + '\n')
        """
        return thought, response

    # å¦‚æœéƒ½æ²¡æœ‰æ‰¾åˆ°æ ¼å¼åŒ–çš„è¾“å‡ºï¼Œè¿”å›åŸå§‹å†…å®¹ä½œä¸º response
    # #region debug log (å·²ç¦ç”¨ç¡¬ç¼–ç è·¯å¾„)
    # with open('/Users/wy770/AI/.cursor/debug.log', 'a') as f:
    #     f.write(json.dumps({
    #         "sessionId": "debug-session",
    #         "runId": "run1",
    #         "hypothesisId": "A",
    #         "location": "agent_stream.py:parse_thought_response:NO_MATCH",
    #         "message": "no pattern matched, returning original content as response",
    #         "data": {
    #             "content_preview": content[:200],
    #             "content_contains_thought": "Thought:" in content or "æ€è€ƒ:" in content or "**Thought**" in content,
    #             "content_contains_response": "Response:" in content or "å›å¤:" in content or "**Response**" in content
    #         },
    #         "timestamp": int(__import__('time').time() * 1000)
    #     }) + '\n')
    # #endregion
    return None, content

from backend.agent.agent.manus import Manus
from backend.agent.schema import AgentState as SchemaAgentState, Message, Role
from backend.agent.web.streaming.events import (
    EventType,
    StreamEvent,
    ThoughtEvent,
    ToolCallEvent,
    ToolResultEvent,
    AnswerEvent,
    AgentStartEvent,
    AgentEndEvent,
    AgentErrorEvent,
    SystemEvent,
)
from backend.agent.web.streaming.agent_state import AgentState, StateInfo
from backend.agent.web.streaming.state_machine import AgentStateMachine

logger = logging.getLogger(__name__)


EventSender = Callable[[dict[str, Any]], asyncio.Task]

# åˆ†æç»“æœæ ‡è®°
ANALYSIS_RESULT_MARKERS = [
    "ğŸ“Š åˆ†æç»“æœæ‘˜è¦",
    "ğŸ’¡ ä¼˜åŒ–å»ºè®®",
    "ğŸ¯ æˆ‘æœ€æ¨èçš„ä¼˜åŒ–",
    "æ˜¯å¦è¦åº”ç”¨è¿™ä¸ªä¼˜åŒ–",
    "æ˜¯å¦è¦ä¼˜åŒ–",
    "æ˜¯å¦è¦ä¼˜åŒ–è¿™æ®µæ•™è‚²ç»å†",
    "ç»¼åˆè¯„åˆ†"
]


@dataclass
class StepStreamState:
    step_id: int
    last_stream_text: str = ""
    last_stream_thought: str = ""
    last_stream_response: str = ""
    stream_emitted: bool = False
    final_emitted: bool = False


class AgentStream:
    """Handles streaming agent execution to WebSocket.

    ä½¿ç”¨ä¸åŸå§‹ server.py ç›¸åŒçš„æ‰§è¡Œé€»è¾‘ï¼š
    - æ‰‹åŠ¨æ­¥éª¤å¾ªç¯
    - è°ƒç”¨ agent.step()
    - å‘é€ step, thought, tool_call, tool_result, answer äº‹ä»¶
    - å»é‡ï¼šé˜²æ­¢å‘é€é‡å¤å†…å®¹
    """

    def __init__(
        self,
        agent: Manus,
        session_id: str,
        state_machine: AgentStateMachine,
        event_sender: EventSender,
        chat_history_manager: Optional[Any] = None,
    ) -> None:
        """Initialize the agent stream.

        Args:
            agent: The Manus agent instance
            session_id: Unique session identifier
            state_machine: The state machine for tracking execution
            event_sender: Async function to send events
            chat_history_manager: Optional chat history manager
        """
        self.agent = agent
        self._session_id = session_id
        self._state_machine = state_machine
        self._send_event = event_sender
        self._chat_history_manager = chat_history_manager

        # ğŸš¨ å»é‡ï¼šè·Ÿè¸ªå·²å‘é€çš„å†…å®¹
        self._sent_thoughts: set[str] = set()
        self._sent_tools: set[str] = set()
        self._last_answer_content: str = ""
        self._answer_sent_in_loop: bool = False  # ğŸš¨ è·Ÿè¸ªå¾ªç¯ä¸­æ˜¯å¦å·²å‘é€è¿‡ answer
        self._answer_event_seq: int = 0
        self._emitted_answer_fingerprints: set[str] = set()
        self._final_answer_sent: bool = False
        self._current_step_stream_state: Optional[StepStreamState] = None
        self._stream_cancel_event: Optional[asyncio.Event] = None

    def _next_answer_event_seq(self) -> int:
        self._answer_event_seq += 1
        return self._answer_event_seq

    def _build_answer_event(
        self,
        *,
        content: str,
        is_complete: bool,
        delta: Optional[str] = None,
    ) -> Optional[AnswerEvent]:
        content_norm = self._normalize_text(content)
        if not content_norm:
            return None

        if is_complete and self._final_answer_sent:
            return None

        delta_norm = self._normalize_text(delta)
        fingerprint = f"{int(is_complete)}|{content_norm}|{delta_norm}"
        if fingerprint in self._emitted_answer_fingerprints:
            return None

        self._emitted_answer_fingerprints.add(fingerprint)
        self._last_answer_content = content_norm
        if is_complete:
            self._final_answer_sent = True
            self._answer_sent_in_loop = True

        return AnswerEvent(
            content=content,
            delta=delta,
            is_complete=is_complete,
            session_id=self._session_id,
            event_seq=self._next_answer_event_seq(),
        )

    def _ensure_assistant_message(self, content: Optional[str]) -> None:
        """Ensure the assistant message is present in memory for persistence."""
        if not content or not content.strip():
            return
        
        content_clean = content.strip()
        
        # æå– Response éƒ¨åˆ†ï¼ˆå¦‚æœå­˜åœ¨ Thought: ... Response: æ ¼å¼ï¼‰
        content_response_part = content_clean
        if "Response:" in content_clean:
            # å¦‚æœ content åŒ…å« Response:ï¼Œæå– Response: ä¹‹åçš„éƒ¨åˆ†
            content_response_part = content_clean.split("Response:")[-1].strip()
        
        for msg in reversed(self.agent.memory.messages):
            if msg.role == Role.ASSISTANT:
                msg_content = (msg.content or "").strip()
                
                # å®Œå…¨åŒ¹é…
                if msg_content == content_clean:
                    return
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯ Thought + Response æ ¼å¼ï¼Œä¸” Response éƒ¨åˆ†åŒ¹é…
                if "Response:" in msg_content:
                    msg_response_part = msg_content.split("Response:")[-1].strip()
                    # å¦‚æœ content çš„ Response éƒ¨åˆ†ä¸å·²å­˜åœ¨çš„ Response éƒ¨åˆ†ç›¸åŒ
                    if msg_response_part == content_response_part:
                        return
                    # å¦‚æœ content å®Œå…¨ç­‰äºå·²å­˜åœ¨çš„ Response éƒ¨åˆ†
                    if msg_response_part == content_clean:
                        return
                
                # æ£€æŸ¥åå‘ï¼šcontent_clean æ˜¯å¦åŒ…å«åœ¨ msg_content ä¸­ï¼ˆä½œä¸ºå­ä¸²ï¼‰
                if content_clean in msg_content:
                    return
                
                # æ£€æŸ¥åå‘ï¼šmsg_content çš„ Response éƒ¨åˆ†æ˜¯å¦åŒ…å« content_clean
                if "Response:" in msg_content:
                    msg_response_part = msg_content.split("Response:")[-1].strip()
                    if content_clean in msg_response_part:
                        return
                
                break
        
        self.agent.memory.add_message(Message.assistant_message(content))

    @staticmethod
    def _normalize_text(text: Optional[str]) -> str:
        return (text or "").strip()

    def _get_latest_assistant_content(self) -> str:
        for msg in reversed(self.agent.memory.messages):
            if msg.role == "assistant" and msg.content:
                return msg.content
        return ""

    def _should_skip_complete_answer(self, content: str) -> bool:
        state = self._current_step_stream_state
        if not state or not state.final_emitted:
            return False
        return self._normalize_text(content) == self._normalize_text(state.last_stream_text)

    def _serialize_tool_calls(self, tool_calls: Any) -> str:
        """Serialize tool calls for deduplication."""
        if not tool_calls:
            return ""
        normalized: List[Any] = []
        for call in tool_calls:
            if hasattr(call, "model_dump"):
                normalized.append(call.model_dump())
            elif hasattr(call, "dict"):
                normalized.append(call.dict())
            elif isinstance(call, dict):
                normalized.append(call)
            else:
                normalized.append(str(call))
        return json.dumps(normalized, ensure_ascii=False, sort_keys=True, default=str)

    def _dedupe_messages(self, messages: List[Message]) -> List[Message]:
        """å»é‡æ¶ˆæ¯åˆ—è¡¨ï¼Œé˜²æ­¢é‡å¤ä¿å­˜åˆ°å†å²è®°å½•ã€‚"""
        if not messages:
            return messages

        deduped: List[Message] = []
        seen_keys: Set[str] = set()

        for msg in messages:
            if msg.role == Role.USER:
                # ç”¨æˆ·æ¶ˆæ¯å·²åœ¨ stream.py ä¸­å†™å…¥ history
                continue

            content = (msg.content or "").strip()
            if msg.role == Role.ASSISTANT:
                response_part = content
                if "Response:" in content:
                    response_part = content.split("Response:")[-1].strip()

                tool_calls_str = self._serialize_tool_calls(msg.tool_calls)
                key = f"assistant|||{content}|||{tool_calls_str}"
                response_key = None
                if response_part and response_part != content:
                    response_key = f"assistant|||{response_part}|||{tool_calls_str}"

                if key in seen_keys or (response_key and response_key in seen_keys):
                    logger.debug(
                        f"[AgentStream] Skip duplicate assistant message: {content[:50]}..."
                    )
                    continue

                seen_keys.add(key)
                if response_key:
                    seen_keys.add(response_key)

            elif msg.role == Role.TOOL:
                key = f"tool|||{msg.name}|||{msg.tool_call_id}|||{content}"
                if key in seen_keys:
                    logger.debug(
                        f"[AgentStream] Skip duplicate tool message: {msg.name}"
                    )
                    continue
                seen_keys.add(key)
            else:
                key = f"{msg.role}|||{content}"
                if key in seen_keys:
                    continue
                seen_keys.add(key)

            deduped.append(msg)

        return deduped

    async def execute(self, user_message: str) -> AsyncIterator[StreamEvent]:
        """Execute agent with streaming events.

        ä½¿ç”¨æ‰‹åŠ¨æ­¥éª¤å¾ªç¯ï¼Œä¸åŸå§‹ server.py é€»è¾‘ç›¸åŒã€‚

        Args:
            user_message: The user's input message

        Yields:
            StreamEvent instances during execution
        """
        start_memory_len = len(self.agent.memory.messages)
        try:
            # Start state
            await self._state_machine.transition_to(
                AgentState.STARTING,
                message="Starting agent execution",
                data={"user_message": user_message},
            )

            # è½¬æ¢ä¸º SSE æ ¼å¼ï¼ˆå‘åå…¼å®¹ï¼‰
            yield AgentStartEvent(
                agent_name="Manus",
                task=user_message,
                session_id=self._session_id,
            )

            # Running state
            await self._state_machine.transition_to(AgentState.RUNNING)

            # ç¡®ä¿æ™ºèƒ½ä½“å¤„äº IDLE çŠ¶æ€
            if self.agent.state != SchemaAgentState.IDLE:
                self.agent.state = SchemaAgentState.IDLE
                self.agent.current_step = 0

            # æ¸…ç†ä¸å®Œæ•´çš„æ¶ˆæ¯åºåˆ—
            self.agent.memory.cleanup_incomplete_sequences()

            # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ° memory
            self.agent.memory.add_message(Message.user_message(user_message))

            # åŒæ­¥åˆ° LangChain Memory
            if hasattr(self.agent, '_langchain_memory') and self.agent._langchain_memory:
                self.agent._langchain_memory.add_user_message(user_message)

            # é‡ç½® answer å‘é€æ ‡å¿—
            self._answer_sent_in_loop = False

            # æ ¹æ®ä»»åŠ¡ç±»å‹åŠ¨æ€è°ƒæ•´æœ€å¤§æ­¥æ•°
            if any(keyword in user_message.lower() for keyword in ["åˆ†æ", "analyze", "æ·±å…¥", "è¯¦ç»†"]):
                max_steps = 10
            else:
                max_steps = 5

            # è®°å½•æœ€åå‘é€çš„æ€è€ƒå†…å®¹
            last_sent_thought = None

            # æ‰‹åŠ¨æ‰§è¡Œæ­¥éª¤å¾ªç¯
            async with self.agent.state_context(SchemaAgentState.RUNNING):
                while self.agent.current_step < max_steps and self.agent.state != SchemaAgentState.FINISHED:
                    if self._state_machine.stop_requested:
                        await self._state_machine.transition_to(AgentState.STOPPED)
                        yield SystemEvent(
                            message="Execution stopped by user",
                            level="info",
                            session_id=self._session_id,
                        )
                        return

                    self.agent.current_step += 1

                    # å‘é€æ­¥éª¤äº‹ä»¶
                    yield SystemEvent(
                        message=f"æ‰§è¡Œæ­¥éª¤ {self.agent.current_step}/{max_steps}",
                        level="info",
                        session_id=self._session_id,
                    )

                    # è®°å½•æ‰§è¡Œå‰çš„æ¶ˆæ¯æ•°é‡
                    msg_count_before = len(self.agent.memory.messages)

                    # çœŸæµå¼ï¼šå¹¶å‘æ‰§è¡Œ step ä¸å†…å®¹æµæ¶ˆè´¹
                    step_state = StepStreamState(step_id=self.agent.current_step)
                    self._current_step_stream_state = step_state
                    # Keep ordered stream chunks to preserve true incremental output.
                    stream_queue: asyncio.Queue[str] = asyncio.Queue()
                    self._stream_cancel_event = asyncio.Event()

                    async def _on_content_delta(content: str) -> None:
                        if not content:
                            return
                        if self._stream_cancel_event and self._stream_cancel_event.is_set():
                            return
                        await stream_queue.put(content)

                    if hasattr(self.agent, "set_stream_content_callback"):
                        self.agent.set_stream_content_callback(
                            _on_content_delta, self._stream_cancel_event
                        )

                    step_task = asyncio.create_task(self.agent.step())
                    step_result: Optional[str] = None
                    try:
                        while not step_task.done() or not stream_queue.empty():
                            if self._state_machine.stop_requested:
                                if self._stream_cancel_event:
                                    self._stream_cancel_event.set()
                                if not step_task.done():
                                    step_task.cancel()
                                break

                            try:
                                streamed_content = await asyncio.wait_for(
                                    stream_queue.get(), timeout=0.01
                                )
                            except asyncio.TimeoutError:
                                continue

                            if (
                                streamed_content
                                and self._normalize_text(
                                    streamed_content
                                    if streamed_content.startswith(step_state.last_stream_text)
                                    else (step_state.last_stream_text + streamed_content)
                                )
                                != self._normalize_text(step_state.last_stream_text)
                            ):
                                # Backward compatibility:
                                # - If callback sends cumulative text, replace directly.
                                # - If callback sends delta text, append incrementally.
                                if streamed_content.startswith(step_state.last_stream_text):
                                    step_state.last_stream_text = streamed_content
                                else:
                                    step_state.last_stream_text += streamed_content
                                step_state.stream_emitted = True

                                # Try to preserve "Thought/Response" UX while streaming.
                                thought_part, response_part = parse_thought_response(
                                    step_state.last_stream_text
                                )
                                if (
                                    thought_part
                                    and self._normalize_text(thought_part)
                                    != self._normalize_text(step_state.last_stream_thought)
                                ):
                                    step_state.last_stream_thought = thought_part
                                    yield ThoughtEvent(
                                        thought=thought_part,
                                        session_id=self._session_id,
                                    )

                                stream_answer = (
                                    response_part
                                    if response_part
                                    else ("" if thought_part else step_state.last_stream_text)
                                )
                                if (
                                    stream_answer
                                    and self._normalize_text(stream_answer)
                                    != self._normalize_text(step_state.last_stream_response)
                                ):
                                    answer_delta = stream_answer
                                    if stream_answer.startswith(step_state.last_stream_response):
                                        answer_delta = stream_answer[
                                            len(step_state.last_stream_response):
                                        ]
                                    step_state.last_stream_response = stream_answer
                                    if answer_delta:
                                        answer_event = self._build_answer_event(
                                            content=stream_answer,
                                            delta=answer_delta,
                                            is_complete=False,
                                        )
                                        if answer_event:
                                            yield answer_event

                        step_result = await step_task
                    except asyncio.CancelledError:
                        logger.info("[AgentStream] step_task cancelled")
                        await self._state_machine.transition_to(AgentState.STOPPED)
                        yield SystemEvent(
                            message="Execution stopped by user",
                            level="info",
                            session_id=self._session_id,
                        )
                        return
                    finally:
                        if hasattr(self.agent, "clear_stream_content_callback"):
                            self.agent.clear_stream_content_callback()
                        self._stream_cancel_event = None

                    logger.info(f"ğŸ” [DEBUG] step() è¿”å›: {step_result}, agent.state: {self.agent.state}, _answer_sent_in_loop: {self._answer_sent_in_loop}")

                    # step æ”¶å°¾ï¼šé¿å…â€œæµå¼æœ«å°¾æ–‡æœ¬â€ä¸â€œmemory æœ€ç»ˆæ–‡æœ¬â€ä¸ä¸€è‡´
                    final_step_content = self._get_latest_assistant_content()
                    if step_state.stream_emitted:
                        # True-streaming single-writer policy:
                        # do NOT emit answer in step-tail. The only writers are:
                        # 1) delta in stream loop
                        # 2) complete=true in FINISHED/fallback branch
                        # Here we only align memory/state to avoid missing persistence.
                        final_candidate = final_step_content or step_state.last_stream_text
                        if final_candidate:
                            thought_part, response_part = parse_thought_response(final_candidate)
                            if (
                                thought_part
                                and self._normalize_text(thought_part)
                                != self._normalize_text(step_state.last_stream_thought)
                            ):
                                step_state.last_stream_thought = thought_part
                                yield ThoughtEvent(
                                    thought=thought_part,
                                    session_id=self._session_id,
                                )

                            final_answer_content = (
                                response_part
                                if response_part
                                else (final_candidate if not thought_part else "")
                            )
                            if final_answer_content:
                                # Ensure final visible answer is persisted even when
                                # assistant memory only contains intermediate/tool messages.
                                self._ensure_assistant_message(final_answer_content)
                                step_state.last_stream_response = final_answer_content

                    # ğŸ” è°ƒè¯•ï¼šæ£€æŸ¥çŠ¶æ€å˜åŒ–
                    # å•ä¸€è·¯å¾„ï¼šå¾ªç¯å†…åªå‘ deltaï¼Œä¸å‘ completeã€‚complete åªåœ¨å¾ªç¯ç»“æŸåå‘ä¸€æ¬¡ã€‚
                    if self.agent.state == SchemaAgentState.FINISHED:
                        logger.info("âœ… Agent çŠ¶æ€å·²è®¾ç½®ä¸º FINISHEDï¼Œé€€å‡ºå¾ªç¯ï¼›complete å°†åœ¨å¾ªç¯ç»“æŸåç»Ÿä¸€å‘é€")
                        break

                    # å®æ—¶å‘é€æ–°å¢çš„æ¶ˆæ¯
                    new_messages = self.agent.memory.messages[msg_count_before:]

                    # æ£€æŸ¥æ˜¯å¦æœ‰åˆ†æå·¥å…·ç»“æœ
                    has_recent_analysis_result = False
                    for msg in reversed(self.agent.memory.messages[-10:]):
                        if msg.role == "tool" and msg.name == 'cv_analyzer_agent':
                            has_recent_analysis_result = True
                            break

                    # å¤„ç†æ–°æ¶ˆæ¯
                    for msg in new_messages:
                        if msg.role == "assistant":
                            # å…ˆå¤„ç† tool_callsï¼ˆassistant æ¶ˆæ¯å¯ä»¥åŒæ—¶æœ‰ content å’Œ tool_callsï¼‰
                            if msg.tool_calls:
                                await self._state_machine.transition_to(AgentState.TOOL_EXECUTING)
                                for tool_call in msg.tool_calls:
                                    tool_name = tool_call.function.name
                                    tool_call_id = tool_call.id  # âœ… è·å– tool_call_id

                                    # ğŸš¨ å»é‡ï¼šä½¿ç”¨ tool_call_id è€Œä¸æ˜¯ step ä½œä¸ºé”®
                                    if tool_call_id in self._sent_tools:
                                        logger.info(f"[è·³è¿‡é‡å¤å·¥å…·] {tool_name} (ID: {tool_call_id[:8]}...)")
                                        continue
                                    self._sent_tools.add(tool_call_id)

                                    tool_args = tool_call.function.arguments
                                    safe_args = str(tool_args).replace("<", r"\<").replace(">", r"\>")
                                    logger.info(f"[å·¥å…·è°ƒç”¨] {tool_name} | ID: {tool_call_id} | å‚æ•°: {safe_args[:100]}...")
                                    yield ToolCallEvent(
                                        tool_name=tool_name,
                                        tool_args=tool_args if isinstance(tool_args, (dict, str)) else {},
                                        session_id=self._session_id,
                                        tool_call_id=tool_call_id,  # âœ… ä¼ é€’ tool_call_id
                                    )

                            # å†å¤„ç† contentï¼ˆå¦‚æœæœ‰ï¼‰
                            if msg.content:
                                # True-streaming å·²ç»é€šè¿‡ on_content_delta è¾“å‡ºè¿‡å¢é‡å†…å®¹ï¼Œ
                                # è¿™é‡Œä¸å†é‡å¤å‘é€ assistant contentï¼Œé¿å… thought/answer é‡å¤ã€‚
                                if step_state.stream_emitted:
                                    logger.debug(
                                        "[AgentStream] Skip assistant content replay in memory loop"
                                    )
                                    continue

                                # ğŸš¨ å»é‡ï¼šè·³è¿‡å·²å‘é€è¿‡çš„ç›¸åŒå†…å®¹
                                content_hash = hash(msg.content)  # ä½¿ç”¨å®Œæ•´å†…å®¹ï¼Œé¿å…æˆªæ–­æ›´æ–°è¢«è¯¯åˆ¤
                                if content_hash in self._sent_thoughts:
                                    logger.debug(f"[è·³è¿‡é‡å¤å†…å®¹] {msg.content[:50]}...")
                                    continue
                                self._sent_thoughts.add(content_hash)

                                # ğŸ¯ è§£æ Thought å’Œ Response æ ¼å¼
                                logger.info(f"[è§£æå‰] åŸå§‹å†…å®¹: {msg.content[:150]}...")

                                # #region debug log
                                import json
            # with open('/Users/wy770/AI/.cursor/debug.log', 'a') as f:
            #     f.write(json.dumps({
            #         "sessionId": "debug-session",
            #         "runId": "run1",
            #         "hypothesisId": "D",
            #         "location": "agent_stream.py:execute:LOOP_BEFORE_PARSE",
            #         "message": "before parse_thought_response in message loop",
            #         "data": {
            #             "msg_content_length": len(msg.content) if msg.content else 0,
            #             "msg_content_preview": msg.content[:300] if msg.content else None
            #         },
            #         "timestamp": int(__import__('time').time() * 1000)
            #     }) + '\n')
                                # #endregion

                                thought_part, response_part = parse_thought_response(msg.content)
                                logger.info(f"[è§£æå] thought={thought_part[:50] if thought_part else None}... response={response_part[:50] if response_part else None}...")

                                # #region debug log
            # with open('/Users/wy770/AI/.cursor/debug.log', 'a') as f:
            #     f.write(json.dumps({
            #         "sessionId": "debug-session",
            #         "runId": "run1",
            #         "hypothesisId": "D",
            #         "location": "agent_stream.py:execute:LOOP_AFTER_PARSE",
            #         "message": "after parse_thought_response in message loop",
            #         "data": {
            #             "thought_part_found": thought_part is not None,
            #             "response_part_found": response_part is not None,
            #             "thought_part_length": len(thought_part) if thought_part else 0,
            #             "response_part_length": len(response_part) if response_part else 0
            #         },
            #         "timestamp": int(__import__('time').time() * 1000)
            #     }) + '\n')
                                # #endregion

                                # åˆ¤æ–­æ˜¯å¦æ˜¯åˆ†æç»“æœå›å¤
                                check_content = response_part or msg.content
                                contains_analysis_result = any(
                                    marker in check_content for marker in ANALYSIS_RESULT_MARKERS
                                )
                                is_final_answer = has_recent_analysis_result and contains_analysis_result

                                # å…ˆå‘é€ Thoughtï¼ˆå¦‚æœæœ‰ï¼‰
                                if thought_part:
                                    logger.info(f"[Thought Process] {thought_part[:100]}...")
                                    # #region debug log
            # with open('/Users/wy770/AI/.cursor/debug.log', 'a') as f:
            #     f.write(json.dumps({
            #         "sessionId": "debug-session",
            #         "runId": "run1",
            #         "hypothesisId": "D",
            #         "location": "agent_stream.py:execute:LOOP_YIELD_THOUGHT",
            #         "message": "yielding ThoughtEvent in message loop",
            #         "data": {
            #             "thought_length": len(thought_part),
            #             "thought_preview": thought_part[:100]
            #         },
            #         "timestamp": int(__import__('time').time() * 1000)
            #     }) + '\n')
                                    # #endregion

                                    # ç”Ÿæˆ CLTP content(channel='think') chunk
                                    # å…³é”®ï¼šä¿æŒæ–‡æœ¬å†…å®¹åŸæ ·ï¼Œä¸è¿›è¡Œä»»ä½•ä¿®æ”¹
                                    # è½¬æ¢ä¸º SSE æ ¼å¼ï¼ˆå‘åå…¼å®¹ï¼‰
                                    yield ThoughtEvent(
                                        thought=thought_part,
                                        session_id=self._session_id,
                                    )
                                else:
                                    # #region debug log (å·²ç¦ç”¨ç¡¬ç¼–ç è·¯å¾„)
                                    # with open('/Users/wy770/AI/.cursor/debug.log', 'a') as f:
                                    #     f.write(json.dumps({
                                    #         "sessionId": "debug-session",
                                    #         "runId": "run1",
                                    #         "hypothesisId": "D",
                                    #         "location": "agent_stream.py:execute:LOOP_NO_THOUGHT",
                                    #         "message": "thought_part is None in message loop, not yielding ThoughtEvent",
                                    #         "data": {},
                                    #         "timestamp": int(__import__('time').time() * 1000)
                                    #     }) + '\n')
                                    # #endregion
                                    pass

                                # Single-writer policy (Karis-aligned):
                                # memory loop no longer emits plain answer.
                                # plain comes from stream delta path + FINISHED/fallback completion only.
                                if response_part:
                                    if is_final_answer:
                                        logger.info(f"[åˆ†æç»“æœå›å¤å€™é€‰] {response_part[:200]}...")
                                elif not thought_part:
                                    # æ²¡æœ‰æ ¼å¼åŒ–è¾“å‡ºæ—¶ä»…ä¿ç•™ thought å…¼å®¹å±•ç¤ºï¼Œä¸åœ¨æ­¤å¤„å‘é€ answer
                                    logger.debug(f"[æ€è€ƒè¿‡ç¨‹] {msg.content[:100]}...")
                                    yield ThoughtEvent(
                                        thought=msg.content,
                                        session_id=self._session_id,
                                    )

                        elif msg.tool_calls:
                            # é assistant æ¶ˆæ¯çš„ tool_callsï¼ˆfallbackï¼‰
                            await self._state_machine.transition_to(AgentState.TOOL_EXECUTING)
                            for tool_call in msg.tool_calls:
                                tool_name = tool_call.function.name
                                tool_call_id = tool_call.id  # âœ… è·å– tool_call_id
                                # ğŸš¨ å»é‡ï¼šä½¿ç”¨ tool_call_id è€Œä¸æ˜¯ step ä½œä¸ºé”®
                                if tool_call_id in self._sent_tools:
                                    logger.info(f"[è·³è¿‡é‡å¤å·¥å…·] {tool_name} (ID: {tool_call_id[:8]}...)")
                                    continue
                                self._sent_tools.add(tool_call_id)

                                tool_args = tool_call.function.arguments
                                safe_args = str(tool_args).replace("<", r"\<").replace(">", r"\>")
                                logger.info(f"[å·¥å…·è°ƒç”¨] {tool_name} | ID: {tool_call_id} | å‚æ•°: {safe_args[:100]}...")
                                yield ToolCallEvent(
                                    tool_name=tool_name,
                                    tool_args=tool_args if isinstance(tool_args, (dict, str)) else {},
                                    session_id=self._session_id,
                                    tool_call_id=tool_call_id,  # âœ… ä¼ é€’ tool_call_id
                                )

                        elif msg.role == "tool":
                            # Only transition if not already in THINKING state
                            if self._state_machine.current_state != AgentState.THINKING:
                                await self._state_machine.transition_to(AgentState.THINKING)
                            content = msg.content
                            tool_call_id = msg.tool_call_id  # âœ… è·å– tool_call_id
                            tool_name = msg.name or "unknown"

                            # æ¸…ç†å‰ç¼€
                            if content and content.startswith("Observed output of cmd `"):
                                prefix_pattern = r"Observed output of cmd `[^`]+` executed:\n"
                                content = re.sub(prefix_pattern, "", content, count=1)
                            elif content and content.startswith("Cmd `"):
                                content = "å·¥å…·æ‰§è¡Œå®Œæˆï¼Œæ— è¾“å‡ºå†…å®¹"

                            # é™åˆ¶æ˜¾ç¤ºé•¿åº¦
                            if content and len(content) > 5000:
                                content = content[:5000] + f"\n...(å†…å®¹å·²æˆªæ–­ï¼Œå…±{len(msg.content)}å­—ç¬¦)"

                            logger.info(f"[å·¥å…·ç»“æœ] {tool_name} | ID: {tool_call_id} | é•¿åº¦: {len(msg.content) if msg.content else 0} å­—ç¬¦")
                            structured_data = None
                            if tool_name in {"web_search", "show_resume"} and hasattr(
                                self.agent, "get_structured_tool_result"
                            ):
                                structured_data = self.agent.get_structured_tool_result(
                                    tool_call_id
                                )
                            yield ToolResultEvent(
                                tool_name=tool_name,
                                result=content or "",
                                is_error=False,
                                session_id=self._session_id,
                                tool_call_id=tool_call_id,  # âœ… ä¼ é€’ tool_call_id
                                structured_data=structured_data,
                            )
                            
                            # ğŸ”‘ å…³é”®ä¿®å¤ï¼šå¦‚æœæ‰§è¡Œäº† terminate å·¥å…·ï¼Œä¸”è¿˜æ²¡æœ‰å‘é€è¿‡ answer
                            # ä¸è¦å°†æŠ€æœ¯æ€§çš„ terminate æ¶ˆæ¯ä½œä¸ºæœ€ç»ˆç­”æ¡ˆï¼Œè€Œæ˜¯è·³è¿‡æˆ–ä½¿ç”¨å‹å¥½æ¶ˆæ¯
                            if tool_name == "terminate" and not self._answer_sent_in_loop:
                                logger.info(f"ğŸ” [DEBUG] terminate å·¥å…·æ‰§è¡Œå®Œæˆï¼Œä½†æ²¡æœ‰ answerï¼Œè·³è¿‡æŠ€æœ¯æ€§æ¶ˆæ¯")
                                # æ ‡è®°å·²å‘é€ï¼Œé˜²æ­¢åç»­å†æ¬¡å¤„ç†ï¼Œä½†ä¸å®é™…å‘é€æŠ€æœ¯æ€§æ¶ˆæ¯ç»™ç”¨æˆ·
                                self._answer_sent_in_loop = True

                    # æ£€æŸ¥æ˜¯å¦é™·å…¥å¾ªç¯
                    if self.agent.is_stuck():
                        logger.info("âš ï¸ Agent æ£€æµ‹åˆ°å¾ªç¯ï¼Œç»ˆæ­¢æ‰§è¡Œ")
                        break

                    # æ£€æŸ¥åˆ†æä»»åŠ¡æ˜¯å¦å®Œæˆ
                    if has_recent_analysis_result:
                        has_analysis_output = False
                        for msg in reversed(self.agent.memory.messages[-10:]):
                            if msg.role == "assistant" and msg.content:
                                contains_result = any(
                                    marker in msg.content for marker in ANALYSIS_RESULT_MARKERS
                                )
                                has_content = len(msg.content) > 100
                                no_more_tools = not msg.tool_calls or len(msg.tool_calls) == 0
                                if contains_result and has_content and no_more_tools:
                                    has_analysis_output = True
                                    logger.info(f"âœ… åˆ†æç»“æœå·²è¾“å‡º: {msg.content[:100]}...")
                                    break

                        if has_analysis_output:
                            logger.info("âœ… åˆ†æä»»åŠ¡å®Œæˆï¼Œç»ˆæ­¢å¾ªç¯")
                            self.agent.state = SchemaAgentState.FINISHED
                            break

            # é‡ç½®æ­¥éª¤è®¡æ•°
            self.agent.current_step = 0
            self.agent.state = SchemaAgentState.IDLE

            # å•ä¸€è·¯å¾„ï¼šåªåœ¨æ­¤å¤„å‘ä¸€æ¬¡ is_complete=Trueï¼ˆæµå¼é˜¶æ®µåªå‘ deltaï¼‰
            final_answer = None
            for msg in reversed(self.agent.memory.messages):
                if msg.role == "assistant" and msg.content:
                    final_answer = msg.content
                    break
            if not final_answer:
                has_terminate = any(
                    m.role == "tool" and m.name == "terminate"
                    for m in self.agent.memory.messages
                )
                if has_terminate:
                    last_user = ""
                    for m in reversed(self.agent.memory.messages):
                        if m.role == "user":
                            last_user = getattr(m, "content", "") or ""
                            break
                    greeting_patterns = ["ä½ å¥½", "hello", "hi", "å—¨", "å“ˆå–½", "æ—©ä¸Šå¥½", "ä¸‹åˆå¥½", "æ™šä¸Šå¥½"]
                    if any(p in (last_user or "").lower() for p in greeting_patterns):
                        final_answer = "ä½ å¥½ï¼æˆ‘æ˜¯ AI åŠ©æ‰‹ï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ï¼æˆ‘å¯ä»¥å¸®åŠ©ä½ å¤„ç†å„ç§ä»»åŠ¡ï¼Œæ¯”å¦‚æœç´¢ä¿¡æ¯ã€ç”ŸæˆæŠ¥å‘Šã€ä¼˜åŒ–ç®€å†ç­‰ã€‚æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®ä½ çš„å—ï¼Ÿ"
                    else:
                        final_answer = "å¥½çš„ï¼Œè¿˜æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ"
            if not final_answer:
                final_answer = "é”™è¯¯ï¼šAgent æ‰§è¡Œè¿‡ç¨‹ä¸­æœªç”Ÿæˆæœ‰æ•ˆå›å¤ã€è¯·æ£€æŸ¥ä»»åŠ¡é…ç½®æˆ–é‡è¯•ã€‚"

            # è§£æ Thought/Responseï¼šåœ¨å”¯ä¸€æ”¶å°¾ç‚¹è¡¥å‘ thoughtï¼ˆè‹¥å°šæœªå‘è¿‡ï¼‰
            thought_part, response_part = parse_thought_response(final_answer)
            if thought_part:
                thought_hash = hash(self._normalize_text(thought_part))
                if thought_hash not in self._sent_thoughts:
                    self._sent_thoughts.add(thought_hash)
                    yield ThoughtEvent(
                        thought=thought_part,
                        session_id=self._session_id,
                    )

            # plain ä»…å– response æ­£æ–‡ï¼Œé¿å… thought æ··å…¥ç­”æ¡ˆ
            final_content = (response_part or final_answer).strip() or final_answer
            self._ensure_assistant_message(final_content)

            if not self._final_answer_sent and not self._should_skip_complete_answer(final_content):
                answer_event = self._build_answer_event(
                    content=final_content,
                    is_complete=True,
                )
                if answer_event:
                    yield answer_event

            # ä¿å­˜åˆ°å†å²è®°å½• - ä¿å­˜æ‰€æœ‰ç±»å‹çš„æ¶ˆæ¯ï¼ˆåŒ…æ‹¬ Tool æ¶ˆæ¯ï¼‰
            if self._chat_history_manager:
                # ä»…ä¿å­˜æœ¬æ¬¡æ‰§è¡Œè¿‡ç¨‹ä¸­æ–°å¢çš„æ¶ˆæ¯ï¼ˆé¿å…é‡å¤ä¿å­˜å†å²æ¶ˆæ¯ï¼‰
                new_messages = self.agent.memory.messages[start_memory_len:]

                deduped_messages = self._dedupe_messages(new_messages)

                for msg in deduped_messages:
                    if msg.role == Role.USER:
                        # ç”¨æˆ·æ¶ˆæ¯å·²åœ¨ stream.py ä¸­å†™å…¥ history
                        continue

                    # ä¿å­˜ assistant æ¶ˆæ¯ï¼ˆå¯èƒ½åŒ…å« tool_callsï¼‰
                    if msg.role == Role.ASSISTANT:
                        # ğŸ”‘ å…³é”®ä¿®å¤ï¼šåªä¿å­˜æœ‰å®é™…å†…å®¹çš„ assistant æ¶ˆæ¯
                        # è¿‡æ»¤æ‰åªæœ‰ tool_calls ä½† content ä¸ºç©ºçš„æ¶ˆæ¯ï¼ˆæŠ€æœ¯æ€§æ¶ˆæ¯ï¼‰
                        has_content = msg.content and len(msg.content.strip()) > 0
                        has_tool_calls = msg.tool_calls and len(msg.tool_calls) > 0

                        if has_content or has_tool_calls:
                            self._chat_history_manager.add_message(Message(
                                role=Role.ASSISTANT,
                                content=msg.content,
                                tool_calls=msg.tool_calls
                            ), persist=False)
                        else:
                            logger.debug(
                                f"[AgentStream] è·³è¿‡ç©ºçš„ assistant æ¶ˆæ¯ (æ—  content ä¸”æ—  tool_calls)"
                            )
                    # ä¿å­˜ tool æ¶ˆæ¯ï¼ˆå…³é”®ï¼šåŒ…å« optimization_suggestions JSONï¼‰
                    elif msg.role == Role.TOOL:
                        self._chat_history_manager.add_message(Message(
                            role=Role.TOOL,
                            content=msg.content,
                            name=msg.name,
                            tool_call_id=msg.tool_call_id
                        ), persist=False)
                        logger.debug(f"  ğŸ’¾ ä¿å­˜ Tool æ¶ˆæ¯: {msg.name}, é•¿åº¦: {len(msg.content or '')}")

                # ğŸ”‘ å…³é”®ä¿®å¤ï¼šåœ¨æ‰€æœ‰æ¶ˆæ¯æ·»åŠ å®Œæˆåï¼Œæ‰‹åŠ¨è§¦å‘ä¸€æ¬¡æŒä¹…åŒ–
                # è¿™ç¡®ä¿ç”¨æˆ·æ¶ˆæ¯ï¼ˆåœ¨ stream.py ä¸­æ·»åŠ  persist=Falseï¼‰å’Œæ‰€æœ‰å…¶ä»–æ¶ˆæ¯éƒ½è¢«ä¿å­˜
                self._chat_history_manager._persist_if_needed()

                logger.info(
                    f"ğŸ“œ å·²ä¿å­˜å¯¹è¯åˆ° ChatHistory (æ–°å¢ {len(deduped_messages)} æ¡æ¶ˆæ¯, "
                    f"æ€»å†…å­˜ {len(self.agent.memory.messages)} æ¡)"
                )

            # Completed state
            await self._state_machine.transition_to(
                AgentState.COMPLETED,
                message="Agent execution completed",
            )

            yield AgentEndEvent(
                agent_name="Manus",
                success=True,
                session_id=self._session_id,
            )

        except Exception as e:
            logger.exception(f"Error during agent execution: {e}")
            await self._state_machine.handle_error(e)
            if self._chat_history_manager:
                self._chat_history_manager.add_message(
                    Message.assistant_message(f"Agentè¿è¡Œå¤±è´¥ï¼š{str(e)}")
                )
            yield AgentErrorEvent(
                error_message=str(e),
                error_type=type(e).__name__,
                session_id=self._session_id,
            )

    async def send_event(self, event: StreamEvent) -> None:
        """Send an event to the client.

        Args:
            event: The event to send
        """
        try:
            task = self._send_event(event.to_dict())
            await asyncio.gather(task, return_exceptions=True)
        except Exception as e:
            logger.error(f"Error sending event: {e}")


class StreamProcessor:
    """Processes streaming agent output for multiple clients.

    Features:
    - Manage multiple active streams
    - Route events to correct clients
    - Handle stream lifecycle
    """

    def __init__(self) -> None:
        """Initialize the stream processor."""
        self._active_streams: dict[str, AgentStream] = {}
        self._lock = asyncio.Lock()

    async def start_stream(
        self,
        session_id: str,
        agent: Manus,
        state_machine: AgentStateMachine,
        event_sender: EventSender,
        user_message: str,
        chat_history_manager: Optional[Any] = None,
    ) -> AsyncIterator[StreamEvent]:
        """Start a new agent stream.

        Args:
            session_id: Unique session identifier
            agent: The Manus agent instance
            state_machine: The state machine for tracking
            event_sender: Function to send events
            user_message: The user's input
            chat_history_manager: Optional chat history manager

        Yields:
            StreamEvent instances during execution
        """
        stream = AgentStream(agent, session_id, state_machine, event_sender, chat_history_manager)

        async with self._lock:
            self._active_streams[session_id] = stream

        # Execute stream and yield events
        try:
            async for event in stream.execute(user_message):
                yield event
        finally:
            await self.remove_stream(session_id)

    async def remove_stream(self, session_id: str) -> None:
        """Remove a completed stream.

        Args:
            session_id: The session ID whose stream to remove
        """
        async with self._lock:
            self._active_streams.pop(session_id, None)

    def has_active_stream(self, session_id: str) -> bool:
        """Check if a session has an active stream.

        Args:
            session_id: The session ID to check

        Returns:
            True if stream is active
        """
        return session_id in self._active_streams

    def get_stream(self, session_id: str) -> Optional[AgentStream]:
        """Get an active stream.

        Args:
            session_id: The session ID

        Returns:
            The AgentStream if active, None otherwise
        """
        return self._active_streams.get(session_id)

    async def stop_stream(self, session_id: str) -> bool:
        """Request a stream to stop.

        Args:
            session_id: The session ID whose stream to stop

        Returns:
            True if stream was found and stop requested
        """
        stream = self.get_stream(session_id)
        if stream:
            stream._state_machine.request_stop()
            return True
        return False
