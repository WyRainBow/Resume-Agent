# 文本太长分块策略与问题复盘

## 1. 遇到的问题

在处理用户上传的包含详细实习经历的长简历文本（约 2000 字符）时，遇到了以下问题：

*   **现象**：后端接口返回 `500 Internal Server Error`，错误日志显示 JSON 解析失败（`Expecting ',' delimiter...`），查看原始内容发现 LLM 返回的 JSON 字符串在末尾被截断。
*   **原因**：
    1.  **输出 Token 限制**：虽然现在的 LLM（如 Doubao Pro/Lite）支持较长的 Context Window（输入），但**输出 Token**（Output Tokens）通常有严格限制（例如 4096 tokens）。简历解析任务是“信息提取”，输出的 JSON 内容长度与输入文本长度高度相关。当输入文本过长时，生成的 JSON 往往会超过输出限制，导致响应被强行截断。
    2.  **分块逻辑失效**：项目中原本存在的 `split_resume_text` 函数虽然设计了 `max_chunk_size` 参数，但内部实现仅依赖“关键词”（如“实习经历”、“项目经验”）进行段落分割。对于一个包含多段实习经历且每段都很详细的“超大段落”，该函数无法将其切开，导致这一整块依然过大。

## 2. 解决方案

采取了**“分而治之”**的策略，具体步骤如下：

### 2.1 优化分块算法
修改 `backend/chunk_processor.py` 中的 `split_resume_text` 函数，引入**强制长度切分**逻辑：
*   **原逻辑**：仅在检测到“关键词”（如 `教育经历`）时切分。
*   **新逻辑**：在遍历行时，累加当前块的字符长度。一旦累积长度超过 `max_chunk_size`（设定为 300），即使没有遇到新关键词，也强制将当前内容作为一个独立块输出，并重置缓冲区。

### 2.2 调整调用策略
修改 `backend/routes/resume.py` 中的 `parse_resume_text` 接口：
*   **阈值判断**：当输入文本长度 > 800 字符时，自动启用分块模式。
*   **参数设置**：调用分块函数时设置 `max_chunk_size=300`。
*   **容错处理**：对每个块的 LLM 调用进行独立的 `try-catch`，确保某个块解析失败不会导致整体崩溃。日志记录具体的 `Chunk Error`。

## 3. 长文本切分策略详解

### Q1: 长文本怎么切分？
采用 **“语义优先，长度兜底”** 的混合切分策略：

1.  **第一层：语义切分（Semantic Splitting）**
    *   利用简历中固定的关键词（`教育经历`, `工作经历`, `项目经验`, `技能`, `奖项`）作为自然分割点。
    *   **优点**：保持了段落内部的上下文完整性，LLM 更容易理解该段落属于哪个 schema 字段。
    *   **缺点**：无法处理单一字段内容过长的情况（例如 10 年工作经验都在“工作经历”这一段）。

2.  **第二层：物理切分（Length-based Splitting）**
    *   在语义段落内部，如果内容长度超过阈值（`max_chunk_size`），则按行（Line-based）进行强制切分。
    *   **优点**：彻底解决了 Token 超限问题，确保每个请求都在 LLM 的“舒适区”。
    *   **缺点**：可能会把一个项目描述切成两半，导致上下文轻微断裂（例如项目名称在上一块，项目细节在下一块）。*注：通过合并逻辑可以缓解这个问题。*

### Q2: 切块大小（Chunk Size）怎么设置？
推荐设置为 **300 - 500 字符**。

*   **为什么不是 2000？**
    *   虽然输入能吃得消，但我们要预留足够的 **输出空间**。
    *   假设输入 500 字符，解析出的 JSON 加上字段名、结构符号，可能会膨胀到 800-1000 tokens。
    *   设置较小的大小（300）可以确保生成的 JSON 即使非常详细，也极难触碰 4096 tokens 的输出上限。
*   **并发优势**：
    *   小块意味着更快的生成速度。
    *   多个小块可以并行调用 LLM（目前代码是串行，未来可优化为并行），显著降低总耗时。

### Q3: 结果怎么合并？
使用 `merge_resume_chunks` 函数处理碎片化的 JSON：

*   **列表（List）**：如 `internships`, `projects`。直接 `extend` 追加。
    *   *Case*：块 A 解析出“腾讯实习”，块 B 解析出“阿里实习”。合并后就是 `[腾讯, 阿里]`。
*   **字典（Dict）**：如 `contact`。使用 `update` 更新。
    *   *Case*：块 A 提取了电话，块 B 提取了邮箱。合并后两者都有。
*   **字符串（String）**：如 `summary`。使用换行符拼接。
    *   *Case*：块 A 有一段总结，块 B 有补充。合并为 `StrA + \n + StrB`。

通过这种策略，我们成功在不牺牲解析精度的前提下，解决了长文本导致的系统稳定性问题。

### Q4: 如何保证切分过程不丢失或混淆关键信息？

这是分块策略中最核心的挑战。我们采用了以下多层保障机制：

#### 4.1 切分时的信息保护

1.  **保留段落标识（Section Label）**
    *   每个切分块都携带其所属的 `section` 标签（如 `实习经历`、`项目经验`）。
    *   即使一个大段落被强制切成多块，每块都会标注相同的 `section`，确保 LLM 知道这些内容属于哪个字段。
    *   **示例**：
        ```python
        chunks = [
            {'section': '实习经历', 'content': '腾讯云...第一部分'},
            {'section': '实习经历', 'content': '...第二部分...深言科技'},
            {'section': '项目经验', 'content': 'AI Agent 项目...'}
        ]
        ```

2.  **按行切分而非按字符切分**
    *   强制切分时以**行（Line）**为最小单位，而不是在句子中间断开。
    *   这确保了每个 bullet point（如 `- 负责xxx功能`）的完整性。
    *   **代码逻辑**：
        ```python
        if current_length >= max_chunk_size:
            # 在行边界切分，不会切断句子
            chunks.append({'section': current_section, 'content': '\n'.join(current_content)})
            current_content = []
        ```

3.  **Prompt 中注入上下文提示**
    *   在调用 LLM 时，Prompt 会明确告知当前块属于哪个段落：
        ```
        从简历文本片段提取信息,只输出JSON:
        片段内容(实习经历):
        {chunk_content}
        ```
    *   这帮助 LLM 理解即使内容不完整，也应该将其归类到正确的字段。

#### 4.2 短文本的处理策略

当文本较短（< 800 字符）时，**不启用分块**，直接走单次 LLM 调用：

*   **原因**：
    1.  短文本不存在 Token 超限风险。
    2.  单次调用保留了完整上下文，解析精度更高。
    3.  避免了不必要的分块开销和合并误差。

*   **阈值选择（800 字符）的依据**：
    *   800 字符的中文文本，经 LLM 解析后生成的 JSON 约 1500-2000 tokens。
    *   远低于 4096 tokens 的输出限制，留有充足余量。
    *   覆盖了大多数"简洁型"简历（应届生、1-2 年经验）。

*   **代码实现**：
    ```python
    if len(body.text) > 800:
        # 长文本：分块处理
        chunks = split_resume_text(body.text, max_chunk_size=300)
        # ...分块调用 LLM 并合并
    else:
        # 短文本：直接处理
        raw = call_llm(provider, prompt)
        short_data = parse_json_response(raw)
    ```

#### 4.3 合并时的信息校验

1.  **去重机制**
    *   对于列表类型字段（如 `internships`），合并时检查是否有重复条目（基于 `title` + `date` 判断）。
    *   避免因切分边界问题导致同一条经历被解析两次。

2.  **空值过滤**
    *   合并前过滤掉空字符串、空数组、`None` 值。
    *   确保最终 JSON 干净整洁。

3.  **字段优先级**
    *   对于单值字段（如 `name`），优先保留第一个非空值。
    *   避免后续块的"幻觉"覆盖正确信息。

#### 4.4 兜底方案：二次校验（可选）

对于高精度要求的场景，可以在合并后增加一轮 **Agent Quick-Fix**：

```python
# 合并后调用 Agent 进行二次校验
fix_response = await fetch('/api/agent/quick-fix', {
    original_text: text,
    current_json: merged_data
})
```

Agent 会对比原始文本和解析结果，修正明显的遗漏或错误（如姓名提取错误、日期格式不一致等）。

---

**总结**：通过"切分时保留上下文 + 短文本直接处理 + 合并时校验去重"的三层机制，我们在保证系统稳定性的同时，最大程度地保障了信息的完整性和准确性。
